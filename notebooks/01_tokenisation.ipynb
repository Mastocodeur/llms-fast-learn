{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9e03276",
   "metadata": {},
   "source": [
    "# Comprendre et manipuler la tokenisation dans les LLM\n",
    "\n",
    "Ce notebook a pour objectif d‚Äôintroduire la tokenisation, une √©tape fondamentale\n",
    "dans le fonctionnement des mod√®les de langage de grande taille (LLM).\n",
    "\n",
    "La tokenisation constitue l‚Äôinterface entre le langage naturel et les mod√®les\n",
    "num√©riques : elle d√©termine comment un texte est d√©coup√©, repr√©sent√© et interpr√©t√©\n",
    "par un mod√®le.\n",
    "\n",
    "√Ä la fin de ce notebook, vous serez capable de :\n",
    "- expliquer ce qu‚Äôest un token et un tokenizer,\n",
    "- comparer diff√©rentes strat√©gies de tokenisation,\n",
    "- manipuler des tokenizers modernes utilis√©s dans les LLM,\n",
    "- comprendre l‚Äôimpact de la tokenisation sur les performances et les co√ªts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986bcba1",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b79ae0",
   "metadata": {},
   "source": [
    "# Pourquoi la tokenisation est n√©cessaire\n",
    "## 1. Du texte au num√©rique\n",
    "\n",
    "Un mod√®le de langage est un mod√®le math√©matique : il ne peut pas traiter directement\n",
    "du texte brut. Toute information doit √™tre repr√©sent√©e sous forme num√©rique.\n",
    "\n",
    "La tokenisation est le processus qui permet de transformer un texte en une s√©quence\n",
    "d‚Äôunit√©s discr√®tes appel√©es *tokens*, lesquelles seront ensuite converties en\n",
    "repr√©sentations num√©riques exploitables par le mod√®le.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1aaf82",
   "metadata": {},
   "source": [
    "## 2. Qu‚Äôest-ce qu‚Äôun token ?\n",
    "\n",
    "Un token est l‚Äôunit√© de base manipul√©e par un mod√®le de langage.\n",
    "Il ne correspond pas n√©cessairement √† un mot entier : il peut s‚Äôagir d‚Äôune partie de\n",
    "mot, d‚Äôun caract√®re, d‚Äôun nombre ou d‚Äôun symbole.\n",
    "\n",
    "Le d√©coupage en tokens d√©pend enti√®rement du *tokenizer* utilis√©.\n",
    "Un m√™me texte peut donc √™tre d√©coup√© diff√©remment selon le mod√®le.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad31d87",
   "metadata": {},
   "source": [
    "## 3. Qu‚Äôest-ce qu‚Äôun tokenizer ?\n",
    "\n",
    "Un tokenizer est le composant charg√© de transformer un texte brut en une s√©quence de tokens selon un ensemble de r√®gles et de param√®tres appris ou d√©finis. \n",
    "\n",
    "Il d√©termine la mani√®re dont le texte est d√©coup√©, quels symboles sont reconnus comme des unit√©s valides et comment ces unit√©s sont associ√©es √† des identifiants num√©riques. \n",
    "\n",
    "Le tokenizer joue un r√¥le central dans le fonctionnement d‚Äôun mod√®le de langage, car il conditionne la repr√©sentation du texte, la taille du vocabulaire, la longueur des s√©quences et, par cons√©quent, les performances et le co√ªt computationnel du mod√®le. \n",
    "\n",
    "Chaque tokenizer est sp√©cifiquement con√ßu pour √™tre compatible avec un mod√®le donn√©, et toute incoh√©rence entre le tokenizer et le mod√®le entra√Æne une d√©gradation significative des r√©sultats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f59420",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986b62e9",
   "metadata": {},
   "source": [
    "# Les diff√©rentes tokenisations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c4a5e6",
   "metadata": {},
   "source": [
    "## Tokenisation na√Øve par mots\n",
    "\n",
    "Le texte est d√©coup√© en mots entiers s√©par√©s par des espaces ou des signes de ponctuation : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e96a89b",
   "metadata": {},
   "source": [
    "![](../images/tokenisation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f19526f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Les', 'mod√®les', 'de', 'langage', 'sont', 'fascinants.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Les mod√®les de langage sont fascinants.\"\n",
    "tokens = text.split()\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8200ff4",
   "metadata": {},
   "source": [
    "Cette approche est simple mais pose rapidement probl√®me :\n",
    "- le vocabulaire devient tr√®s grand,\n",
    "- les mots inconnus ne peuvent pas √™tre repr√©sent√©s,\n",
    "- la g√©n√©ralisation est limit√©e."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860a045d",
   "metadata": {},
   "source": [
    "## Tokenisation par caract√®res\n",
    "\n",
    "Chaque caract√®re du texte est trait√© comme un token, garantissant une couverture compl√®te du vocabulaire au prix de s√©quences tr√®s longues : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27e313de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['L',\n",
       " 'e',\n",
       " 's',\n",
       " ' ',\n",
       " 'm',\n",
       " 'o',\n",
       " 'd',\n",
       " '√®',\n",
       " 'l',\n",
       " 'e',\n",
       " 's',\n",
       " ' ',\n",
       " 'd',\n",
       " 'e',\n",
       " ' ',\n",
       " 'l',\n",
       " 'a',\n",
       " 'n',\n",
       " 'g',\n",
       " 'a',\n",
       " 'g',\n",
       " 'e',\n",
       " ' ',\n",
       " 's',\n",
       " 'o',\n",
       " 'n',\n",
       " 't',\n",
       " ' ',\n",
       " 'f',\n",
       " 'a',\n",
       " 's',\n",
       " 'c',\n",
       " 'i',\n",
       " 'n',\n",
       " 'a',\n",
       " 'n',\n",
       " 't',\n",
       " 's',\n",
       " '.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd50b92e",
   "metadata": {},
   "source": [
    "La tokenisation par caract√®res r√©sout le probl√®me des mots inconnus,\n",
    "mais elle produit des s√©quences tr√®s longues, ce qui augmente fortement\n",
    "le co√ªt computationnel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56aefa78",
   "metadata": {},
   "source": [
    "## Tokenisation par sous-mots\n",
    "\n",
    "Les LLM modernes utilisent majoritairement des tokenizations bas√©es sur des sous-mots.\n",
    "L‚Äôid√©e est de d√©couper les mots en unit√©s fr√©quentes, r√©utilisables, permettant\n",
    "de g√©rer efficacement les mots rares ou inconnus.\n",
    "\n",
    "Exemple :\n",
    "- \"incompr√©hensible\" ‚Üí \"in\" + \"compr√©hens\" + \"ible\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe107bc3",
   "metadata": {},
   "source": [
    "![](../images/tokenisation_illustration.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1f1a3d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['token', '##ization', 'is', 'computational', '##ly', 'fascinating', '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "text = \"Tokenization is computationally fascinating.\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8688896f",
   "metadata": {},
   "source": [
    "On observe que le texte est d√©coup√© en sous-unit√©s, et non en mots stricts. \n",
    "\n",
    "De plus, le pr√©fixe `##` indique que le token est une continuation d‚Äôun mot pr√©c√©dent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e68fd8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 19204, 3989, 2003, 15078, 2135, 17160, 1012, 102]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = tokenizer.encode(text)\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c064e636",
   "metadata": {},
   "source": [
    "Les mod√®les ne manipulent pas directement les tokens textuels, mais leurs identifiants num√©riques associ√©s.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35d0bb7",
   "metadata": {},
   "source": [
    "## Tokenisation au niveau des bytes (byte-level tokenization)  \n",
    "\n",
    "Le texte est tokenis√© √† partir des octets qui le composent, assurant une repr√©sentation universelle et ind√©pendante de la langue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eae754e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Prix : 199,99\\xe2\\x82\\xac \\xf0\\x9f\\x98\\x84'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Prix : 199,99‚Ç¨ üòÑ\"\n",
    "\n",
    "# Encodage UTF-8 en bytes\n",
    "byte_tokens = text.encode(\"utf-8\")\n",
    "\n",
    "byte_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ec46ba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[80,\n",
       " 114,\n",
       " 105,\n",
       " 120,\n",
       " 32,\n",
       " 58,\n",
       " 32,\n",
       " 49,\n",
       " 57,\n",
       " 57,\n",
       " 44,\n",
       " 57,\n",
       " 57,\n",
       " 226,\n",
       " 130,\n",
       " 172,\n",
       " 32,\n",
       " 240,\n",
       " 159,\n",
       " 152,\n",
       " 132]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(byte_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c9e2595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Prix : 199,99‚Ç¨ üòÑ'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bytes(byte_tokens).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b532a7a",
   "metadata": {},
   "source": [
    "## Tokenisation hybride \n",
    "\n",
    "Plusieurs niveaux de tokenisation sont combin√©s afin d‚Äô√©quilibrer robustesse, efficacit√© et pr√©cision selon le contexte d‚Äôutilisation.\n",
    "\n",
    "Principe :\n",
    "* on tokenise d‚Äôabord par mots connus,\n",
    "* sinon on d√©coupe en sous-mots,\n",
    "* sinon on retombe au niveau byte ou caract√®re."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d7180a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chat',\n",
       " 'G',\n",
       " 'PT',\n",
       " 'ification',\n",
       " 'ƒ†ultra',\n",
       " '-',\n",
       " 'm',\n",
       " '√É¬©',\n",
       " 'ta',\n",
       " '-',\n",
       " 'quant',\n",
       " 'ique',\n",
       " 'ƒ†√∞≈Åƒ∫',\n",
       " 'ƒ¶']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "text = \"ChatGPTification ultra-m√©ta-quantique üòÑ\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0b387d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[30820,\n",
       " 38,\n",
       " 11571,\n",
       " 2649,\n",
       " 14764,\n",
       " 12,\n",
       " 76,\n",
       " 2634,\n",
       " 8326,\n",
       " 12,\n",
       " 40972,\n",
       " 2350,\n",
       " 30325,\n",
       " 226]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e707136",
   "metadata": {},
   "source": [
    "## Impact pratique de la tokenisation\n",
    "\n",
    "Les limites de contexte des LLM sont exprim√©es en nombre de tokens,\n",
    "pas en nombre de mots ou de caract√®res.\n",
    "\n",
    "Deux textes de longueur similaire peuvent donc avoir des co√ªts tr√®s diff√©rents\n",
    "selon leur tokenisation.\n",
    "\n",
    "Les mod√®les OpenAI utilisent un tokenizer sp√©cifique optimis√© pour leurs mod√®les.\n",
    "La biblioth√®que `tiktoken` permet de reproduire exactement la tokenisation\n",
    "utilis√©e en production.\n",
    "\n",
    "Cela permet notamment :\n",
    "- de compter pr√©cis√©ment le nombre de tokens,\n",
    "- d'estimer le co√ªt d'une requ√™te API,\n",
    "- de v√©rifier la compatibilit√© avec la limite de contexte.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "49960310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Every moment is a beginning\n",
      "Token ids: [15745, 4205, 382, 261, 10526]\n",
      "Number of tokens: 5\n",
      "Decoded text: Every moment is a beginning\n",
      "Nombre de mots: 5\n",
      "Nombre de caract√®res: 27\n",
      "Estimation du co√ªt (exemple fictif) : $0.00077500\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# Charger le tokenizer associ√© au mod√®le\n",
    "tokenizer = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "\n",
    "# Exemple de texte √† tokeniser\n",
    "sample_text = \"Every moment is a beginning\"\n",
    "\n",
    "# Tokeniser le texte\n",
    "token_ids = tokenizer.encode(sample_text)\n",
    "\n",
    "# Afficher les r√©sultats\n",
    "print(\"Text:\", sample_text)\n",
    "print(\"Token ids:\", token_ids)\n",
    "print(\"Number of tokens:\", len(token_ids))\n",
    "\n",
    "# D√©coder les tokens pour v√©rifier la correspondance avec le texte original\n",
    "decoded_text = tokenizer.decode(token_ids)\n",
    "print(\"Decoded text:\", decoded_text)\n",
    "\n",
    "print(\"Nombre de mots:\", len(sample_text.split()))\n",
    "print(\"Nombre de caract√®res:\", len(sample_text))\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Estimation du co√ªt (exemple p√©dagogique concret)\n",
    "# -------------------------------------------------------\n",
    "\n",
    "# Supposons un tarif fictif mais r√©aliste :\n",
    "# 5 $ par million de tokens en entr√©e\n",
    "# 15 $ par million de tokens en sortie\n",
    "\n",
    "price_input_per_million = 5.0\n",
    "price_output_per_million = 15.0\n",
    "\n",
    "input_tokens = len(token_ids)\n",
    "output_tokens = 50  # Supposons que le mod√®le g√©n√®re 50 tokens en r√©ponse\n",
    "\n",
    "estimated_cost = (\n",
    "    (input_tokens / 1_000_000) * price_input_per_million\n",
    "    + (output_tokens / 1_000_000) * price_output_per_million\n",
    ")\n",
    "\n",
    "print(f\"Estimation du co√ªt (exemple fictif) : ${estimated_cost:.8f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821ddc47",
   "metadata": {},
   "source": [
    "Ici : \n",
    "- Le texte est converti en une s√©quence d'identifiants num√©riques.\n",
    "- Chaque identifiant correspond √† un token du vocabulaire du mod√®le.\n",
    "- Le nombre de tokens n'est pas n√©cessairement √©gal au nombre de mots.\n",
    "\n",
    "Les API de mod√®les de langage facturent g√©n√©ralement au nombre de tokens.\n",
    "Nous illustrons ici une estimation de co√ªt en utilisant un exemple tarifaire fictif\n",
    "(les prix r√©els d√©pendent du mod√®le et peuvent √©voluer)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4902e728",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433da02b",
   "metadata": {},
   "source": [
    "# Comparaison de tokenizers\n",
    "\n",
    "Tous les mod√®les de langage ne tokenisent pas le texte de la m√™me mani√®re.\n",
    "Chaque mod√®le est entra√Æn√© avec un tokenizer sp√©cifique, bas√© sur des choix\n",
    "techniques diff√©rents (WordPiece, Byte-Level BPE, SentencePiece, etc.).\n",
    "\n",
    "Nous allons comparer trois tokenizers associ√©s √† des mod√®les populaires :\n",
    "\n",
    "- BERT (WordPiece)\n",
    "- GPT-2 (Byte-Level BPE)\n",
    "- XLM-RoBERTa (SentencePiece)\n",
    "\n",
    "L'objectif est d'observer :\n",
    "- les diff√©rences de d√©coupage,\n",
    "- le nombre de tokens g√©n√©r√©s,\n",
    "- l'impact des choix de tokenisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e86b630c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenizer: bert-base-uncased\n",
      "['l', \"'\", 'international', '##isation', 'des', 'model', '##es', 'de', 'lang', '##age', 'est', 'complex', '##e', '.']\n",
      "Nombre de tokens: 14\n",
      "\n",
      "Tokenizer: gpt2\n",
      "['L', \"'\", 'international', 'isation', 'ƒ†des', 'ƒ†mod', '√É¬®', 'les', 'ƒ†de', 'ƒ†lang', 'age', 'ƒ†est', 'ƒ†complex', 'e', '.']\n",
      "Nombre de tokens: 15\n",
      "\n",
      "Tokenizer: xlm-roberta-base\n",
      "['‚ñÅL', \"'\", 'international', 'isation', '‚ñÅdes', '‚ñÅmod√®les', '‚ñÅde', '‚ñÅlang', 'age', '‚ñÅest', '‚ñÅcomplexe', '.']\n",
      "Nombre de tokens: 12\n"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "    \"bert-base-uncased\",\n",
    "    \"gpt2\",\n",
    "    \"xlm-roberta-base\"\n",
    "]\n",
    "\n",
    "text = \"L'internationalisation des mod√®les de langage est complexe.\"\n",
    "\n",
    "for model in models:\n",
    "    tok = AutoTokenizer.from_pretrained(model)\n",
    "    print(f\"\\nTokenizer: {model}\")\n",
    "    print(tok.tokenize(text))\n",
    "    print(\"Nombre de tokens:\", len(tok.tokenize(text)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f7981b",
   "metadata": {},
   "source": [
    "## 1. BERT (WordPiece)\n",
    "\n",
    "- Les sous-mots sont indiqu√©s par le pr√©fixe `##`.\n",
    "- Exemple : \"internationalisation\" ‚Üí \"international\" + \"##isation\".\n",
    "- Les accents sont supprim√©s (mod√®les uncased).\n",
    "- Le texte fran√ßais est partiellement d√©grad√© (\"mod√®les\" ‚Üí \"model\" + \"##es\").\n",
    "\n",
    "BERT est optimis√© pour l'anglais, ce qui explique certaines approximations\n",
    "sur le fran√ßais.\n",
    "\n",
    "Nombre de tokens : 14\n",
    "\n",
    "\n",
    "## 2. GPT-2 (Byte-Level BPE)\n",
    "\n",
    "- Les espaces sont encod√©s avec le symbole `ƒ†`.\n",
    "- Les caract√®res accentu√©s sont d√©compos√©s en bytes (\"mod√®les\" ‚Üí \"√É¬®\").\n",
    "- Le mod√®le fonctionne au niveau des bytes, ce qui garantit une couverture universelle.\n",
    "\n",
    "GPT-2 n'a pas √©t√© sp√©cifiquement entra√Æn√© pour le fran√ßais,\n",
    "ce qui explique certains d√©coupages peu naturels.\n",
    "\n",
    "Nombre de tokens : 15\n",
    "\n",
    "\n",
    "## 3. XLM-RoBERTa (SentencePiece)\n",
    "\n",
    "- Le symbole `‚ñÅ` repr√©sente un espace.\n",
    "- Les mots fran√ßais sont mieux conserv√©s (\"mod√®les\", \"complexe\").\n",
    "- Le mod√®le est multilingue, ce qui am√©liore la tokenisation du fran√ßais.\n",
    "\n",
    "Nombre de tokens : 12\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea9a99f",
   "metadata": {},
   "source": [
    "## Cas limites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4cfd41da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatGPTification ultra-m√©ta-quantique\n",
      "['Chat', 'G', 'PT', 'ification', 'ƒ†ultra', '-', 'm', '√É¬©', 'ta', '-', 'quant', 'ique']\n",
      "\n",
      "Prix : 199,99‚Ç¨ üòÑ\n",
      "['P', 'rix', 'ƒ†:', 'ƒ†199', ',', '99', '√¢ƒ§¬¨', 'ƒ†√∞≈Åƒ∫', 'ƒ¶']\n",
      "\n",
      "anticonstitutionnellement\n",
      "['ant', 'icon', 'st', 'itution', 'nel', 'lement']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    \"ChatGPTification ultra-m√©ta-quantique\",\n",
    "    \"Prix : 199,99‚Ç¨ üòÑ\",\n",
    "    \"anticonstitutionnellement\"\n",
    "]\n",
    "\n",
    "for t in texts:\n",
    "    print(t)\n",
    "    print(tokenizer.tokenize(t))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a8f2de",
   "metadata": {},
   "source": [
    "Ces exemples illustrent plusieurs propri√©t√©s importantes du tokenizer GPT-2 :\n",
    "\n",
    "1. Les mots inconnus ou n√©ologismes sont d√©coup√©s en sous-unit√©s connues.\n",
    "2. Les caract√®res accentu√©s, symboles et emojis sont repr√©sent√©s via un encodage byte-level.\n",
    "3. Les mots longs sont segment√©s en fragments morphologiques fr√©quents.\n",
    "\n",
    "Cela montre que la tokenisation moderne vise un compromis entre :\n",
    "- robustesse universelle (via les bytes),\n",
    "- efficacit√© du vocabulaire,\n",
    "- capacit√© de g√©n√©ralisation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4347cb",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d66ed14",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "La tokenisation est une √©tape fondamentale dans les LLM.\n",
    "Elle conditionne la repr√©sentation du texte, la longueur des s√©quences,\n",
    "les performances des mod√®les et leur co√ªt d‚Äôutilisation.\n",
    "\n",
    "Comprendre la tokenisation, c‚Äôest comprendre comment un mod√®le de langage\n",
    "lit r√©ellement le langage humain.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4218edb2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
