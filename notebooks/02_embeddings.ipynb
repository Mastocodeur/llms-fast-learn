{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25c80162",
   "metadata": {},
   "source": [
    "# Comprendre et manipuler les embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3fdf43",
   "metadata": {},
   "source": [
    "Dans le notebook précédent, nous avons vu comment un texte est transformé en tokens puis en identifiants numériques.\n",
    "\n",
    "Ces identifiants sont arbitraires : ils ne contiennent aucune information sémantique.\n",
    "\n",
    "Dans ce notebook, nous allons comprendre comment ces identifiants deviennent des vecteurs numériques capables de représenter le sens.\n",
    "\n",
    "Nous commencerons par les word embeddings classiques avec Word2Vec, puis nous ferons la transition vers les embeddings utilisés dans les modèles de langage modernes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1204b57c",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c286184a",
   "metadata": {},
   "source": [
    "## Pourquoi les embeddings ?\n",
    "\n",
    "Considérons les token IDs suivants : [15745, 4205, 382, 261, 10526]\n",
    "\n",
    "Ces nombres n’ont aucune relation mathématique entre eux. Le modèle a besoin d’une représentation dense et structurée pour apprendre des relations entre mots.\n",
    "\n",
    "Un embedding est un vecteur dense associé à chaque token.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d725794d",
   "metadata": {},
   "source": [
    "## Matrice d'embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "234c7361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.43770902, 0.42238258, 0.49433079, 0.71719917],\n",
       "       [0.03416924, 0.34834692, 0.89392209, 0.84209969],\n",
       "       [0.52988088, 0.03042663, 0.55453657, 0.1743212 ],\n",
       "       [0.61297467, 0.63432835, 0.16289286, 0.33030735],\n",
       "       [0.1597706 , 0.25827432, 0.80620816, 0.46777945]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Supposons un vocabulaire de 5 tokens\n",
    "vocab_size = 5\n",
    "embedding_dim = 4\n",
    "\n",
    "# Matrice d'embedding aléatoire\n",
    "embedding_matrix = np.random.rand(vocab_size, embedding_dim)\n",
    "\n",
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6828415b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: (5, 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"Embedding matrix shape:\", embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c54f46",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844802ca",
   "metadata": {},
   "source": [
    "# Word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9bc4a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# Le modèle word2vec-google-news-300 est volumineux (~1.6 Go).\n",
    "#model = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "# Le modèle glove-wiki-gigaword-50 est plus léger (~70 Mo).\n",
    "model = api.load(\"glove-wiki-gigaword-50\")\n",
    "word_vectors = model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6974a928",
   "metadata": {},
   "source": [
    "Ce modèle associe chaque mot à un vecteur de dimension 50.\n",
    "\n",
    "Il a été entraîné sur Wikipedia et Gigaword.\n",
    "\n",
    "Il est beaucoup plus léger que Word2Vec Google News, mais il permet d’illustrer exactement les mêmes concepts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2122979e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.50451   0.68607  -0.59517  -0.022801  0.60046  -0.13498  -0.08813\n",
      "  0.47377  -0.61798  -0.31012  -0.076666  1.493    -0.034189 -0.98173\n",
      "  0.68229   0.81722  -0.51874  -0.31503  -0.55809   0.66421   0.1961\n",
      " -0.13495  -0.11476  -0.30344   0.41177  -2.223    -1.0756   -1.0783\n",
      " -0.34354   0.33505   1.9927   -0.04234  -0.64319   0.71125   0.49159\n",
      "  0.16754   0.34344  -0.25663  -0.8523    0.1661    0.40102   1.1685\n",
      " -1.0137   -0.21585  -0.15155   0.78321  -0.91241  -1.6106   -0.64426\n",
      " -0.51042 ]\n",
      "Dimension du vecteur : 50\n"
     ]
    }
   ],
   "source": [
    "vector = word_vectors[\"king\"] # On récupère le vecteur d'embedding du mot \"king\"\n",
    "print(vector) # Affiche le vecteur d'embedding du mot \"king\"\n",
    "print(\"Dimension du vecteur :\", len(vector)) # Affiche la dimension du vecteur d'embedding du mot \"king\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9bc28e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.8523604273796082), ('throne', 0.7664334177970886), ('prince', 0.7592144012451172), ('daughter', 0.7473883628845215), ('elizabeth', 0.7460219860076904)]\n"
     ]
    }
   ],
   "source": [
    "result = word_vectors.most_similar( # On cherche les mots les plus similaires selon un critère\n",
    "    positive=[\"king\", \"woman\"], # On cherche un mot qui est à \"king\" et \"woman\"\n",
    "    negative=[\"man\"], # On cherche un mot qui est à \"king\" et \"woman\" mais pas à \"man\"\n",
    "    topn=5 # On veut les 5 mots les plus similaires selon ce critère\n",
    ")\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa63819",
   "metadata": {},
   "source": [
    "Le modèle calcule un nouveau vecteur :\n",
    "\n",
    "```\n",
    "king + woman - man\n",
    "```\n",
    "\n",
    "Puis retourne les mots les plus proches de ce vecteur.\n",
    "\n",
    "Cela montre que certaines relations sémantiques sont encodées\n",
    "comme des directions dans l’espace vectoriel.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6253b84",
   "metadata": {},
   "source": [
    "### Similarité cosinus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a791044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king queen 0.7839043\n",
      "man woman 0.8860338\n",
      "computer keyboard 0.5768126\n",
      "king computer 0.22757\n"
     ]
    }
   ],
   "source": [
    "pairs = [\n",
    "    (\"king\", \"queen\"),\n",
    "    (\"man\", \"woman\"),\n",
    "    (\"computer\", \"keyboard\"),\n",
    "    (\"king\", \"computer\")\n",
    "]\n",
    "\n",
    "for w1, w2 in pairs:\n",
    "    sim = word_vectors.similarity(w1, w2)\n",
    "    print(w1, w2, sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c66da72",
   "metadata": {},
   "source": [
    "La similarité cosinus mesure l’angle entre deux vecteurs.\n",
    "\n",
    "Une valeur proche de 1 indique une forte proximité sémantique.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ff8cbe",
   "metadata": {},
   "source": [
    "### Visualisation simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32060b8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "words = [\"king\", \"queen\", \"man\", \"woman\", \"computer\", \"keyboard\"]\n",
    "\n",
    "vectors = np.array([word_vectors[w] for w in words])\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "reduced = pca.fit_transform(vectors)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(reduced[:, 0], reduced[:, 1])\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, (reduced[i, 0], reduced[i, 1]))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68bf3a8",
   "metadata": {},
   "source": [
    "La projection 2D déforme l’espace réel (qui est en 50 dimensions), mais elle permet d’illustrer que les mots proches sémantiquement tendent à être proches géométriquement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298803fe",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f35784",
   "metadata": {},
   "source": [
    "# Token et positional embeddings avec PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d4ca4d",
   "metadata": {},
   "source": [
    "## Token Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4436753b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.3035],\n",
      "        [-0.5880,  0.3486,  0.6603],\n",
      "        [-0.2196, -0.3792,  0.7671],\n",
      "        [-1.1925,  0.6984, -1.4097],\n",
      "        [ 0.1794,  1.8951,  1.3689],\n",
      "        [-1.6033, -1.3250,  0.1784],\n",
      "        [-2.1338,  1.0524, -0.3885],\n",
      "        [-0.9343,  1.8319, -0.3378],\n",
      "        [ 0.8805,  1.5542,  0.6266],\n",
      "        [-0.1755,  0.0983, -0.0935]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "vocab_size = 10 # Supposons un vocabulaire de 10 tokens\n",
    "embedding_dim = 3 # Chaque token sera représenté par un vecteur de dimension 3\n",
    "\n",
    "torch.manual_seed(123) # Pour la reproductibilité, on fixe la graine aléatoire\n",
    "\n",
    "# Crée une couche d'embedding avec un vocabulaire de 10 tokens et des vecteurs de dimension 3\n",
    "embedding_layer = nn.Embedding(vocab_size, embedding_dim) \n",
    "# Affiche les poids de la couche d'embedding\n",
    "print(embedding_layer.weight)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc3c286",
   "metadata": {},
   "source": [
    "Nous simulons maintenant une séquence de trois tokens représentée par leurs identifiants :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "300aa181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs : tensor([1, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.tensor([1, 5, 8])\n",
    "print(\"Token IDs :\", token_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cff802b",
   "metadata": {},
   "source": [
    "On récupère leurs embeddings :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "078151f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3])\n",
      "tensor([[-0.5880,  0.3486,  0.6603],\n",
      "        [-1.6033, -1.3250,  0.1784],\n",
      "        [ 0.8805,  1.5542,  0.6266]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Récupère les embeddings correspondants aux token IDs\n",
    "token_embeddings = embedding_layer(token_ids)\n",
    "\n",
    "print(token_embeddings.shape) # Affiche la forme des embeddings récupérés\n",
    "print(token_embeddings) # Affiche les embeddings récupérés pour les token IDs [1, 5, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b63f82c",
   "metadata": {},
   "source": [
    "`token_embeddings` est maintenant une matrice de forme (longueur_sequence, dimension_embedding).\n",
    "\n",
    "Chaque ligne correspond à un token de la séquence.\n",
    "Ces vecteurs contiennent l’information sémantique, mais pas l’ordre.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f406c956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.3035],\n",
      "        [-0.5880,  0.3486,  0.6603],\n",
      "        [-0.2196, -0.3792,  0.7671],\n",
      "        [-1.1925,  0.6984, -1.4097],\n",
      "        [ 0.1794,  1.8951,  1.3689],\n",
      "        [-1.6033, -1.3250,  0.1784],\n",
      "        [-2.1338,  1.0524, -0.3885],\n",
      "        [-0.9343,  1.8319, -0.3378],\n",
      "        [ 0.8805,  1.5542,  0.6266],\n",
      "        [-0.1755,  0.0983, -0.0935]], requires_grad=True)\n",
      "\n",
      "\n",
      "tensor([[-1.1925,  0.6984, -1.4097]], grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "\n",
      "tensor([[-1.1925,  0.6984, -1.4097],\n",
      "        [-1.6033, -1.3250,  0.1784],\n",
      "        [-0.9343,  1.8319, -0.3378]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Affiche les poids de la couche d'embedding (les vecteurs d'embedding pour chaque token)\n",
    "print(embedding_layer.weight)\n",
    "print('\\n')\n",
    "# Affiche le vecteur d'embedding pour le token 3\n",
    "print(embedding_layer(torch.tensor([3]))) \n",
    "print('\\n')\n",
    "# Affiche les vecteurs d'embedding pour les tokens 3, 5 et 7\n",
    "print(embedding_layer(torch.tensor([3, 5, 7])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d10d73",
   "metadata": {},
   "source": [
    "Une couche d’embedding est simplement une matrice de paramètres appris.\n",
    "\n",
    "Un identifiant numérique sert d’index pour récupérer une ligne.\n",
    "\n",
    "Contrairement à GloVe, ces vecteurs ne sont pas fixes.\n",
    "Ils sont ajustés pendant l’entraînement du modèle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d855c7e",
   "metadata": {},
   "source": [
    "## Positional Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b863f81",
   "metadata": {},
   "source": [
    "Les token embeddings représentent le sens des mots, mais ils ne contiennent\n",
    "aucune information sur l’ordre dans lequel les mots apparaissent.\n",
    "\n",
    "Considérons les deux phrases suivantes :\n",
    "\n",
    "The dog jumps on the cat.  \n",
    "The cat jumps on the dog.\n",
    "\n",
    "Elles contiennent exactement les mêmes tokens.\n",
    "Pourtant, leur signification est très différente.\n",
    "\n",
    "Si le modèle ne reçoit que des embeddings de tokens,\n",
    "il ne peut pas distinguer ces deux phrases.\n",
    "\n",
    "Il est donc nécessaire d’ajouter une information de position.\n",
    "\n",
    "Pour cela, nous créons maintenant une seconde couche d’embedding, mais cette fois pour représenter les positions dans la séquence.\n",
    "\n",
    "La taille du vocabulaire devient la longueur maximale de la séquence.\n",
    "\n",
    "Chaque position possède son propre vecteur.\n",
    "\n",
    "La dimension doit être identique à celle des embeddings de tokens afin que les deux puissent être additionnés.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a76a2253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position IDs : tensor([0, 1, 2])\n",
      "torch.Size([3, 3])\n",
      "tensor([[-0.9178,  0.9045, -2.0975],\n",
      "        [ 1.1558, -1.2157,  0.1295],\n",
      "        [ 0.0967,  1.4086,  0.1915]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Supposons que nous avons une séquence de longueur 3\n",
    "max_length = 3\n",
    "# Crée une couche d'embedding pour les positions avec une dimension d'embedding de 3\n",
    "pos_embedding_layer = nn.Embedding(max_length, embedding_dim)\n",
    "\n",
    "# Affiche les poids de la couche d'embedding pour les positions\n",
    "position_ids = torch.arange(max_length)\n",
    "# Affiche les IDs de position\n",
    "print(\"Position IDs :\", position_ids)\n",
    "\n",
    "# Récupère les embeddings pour les positions\n",
    "pos_embeddings = pos_embedding_layer(position_ids)\n",
    "\n",
    "# Affiche la forme et les embeddings pour les positions\n",
    "print(pos_embeddings.shape)\n",
    "print(pos_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f486ce",
   "metadata": {},
   "source": [
    "Chaque ligne correspond maintenant à une position dans la séquence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12ac67a",
   "metadata": {},
   "source": [
    "Pour chaque token à la position i :\n",
    "\n",
    "1. On récupère son embedding de token.\n",
    "2. On récupère l’embedding correspondant à sa position.\n",
    "3. On additionne les deux vecteurs élément par élément.\n",
    "\n",
    "Le résultat est un vecteur unique qui encode à la fois\n",
    "le sens du token et sa position dans la phrase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c13e1657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3])\n",
      "tensor([[-1.5058,  1.2531, -1.4372],\n",
      "        [-0.4475, -2.5406,  0.3080],\n",
      "        [ 0.9772,  2.9627,  0.8182]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "\n",
    "print(input_embeddings.shape)\n",
    "print(input_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a79c415",
   "metadata": {},
   "source": [
    "Les vecteurs résultants sont les véritables entrées du Transformer.\n",
    "\n",
    "Le modèle ne manipule plus :\n",
    "- des identifiants de tokens\n",
    "- ni des indices de position\n",
    "\n",
    "Il travaille uniquement avec des vecteurs combinant\n",
    "information sémantique et information d’ordre.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d5fd19",
   "metadata": {},
   "source": [
    "## Exemple d'application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa09e898",
   "metadata": {},
   "source": [
    "Prenons une phrase très simple : `Le chat dort`\n",
    "\n",
    "Imaginons que notre tokenizer ait déjà transformé cette phrase en identifiants : \n",
    "\n",
    "Le    → 2  \n",
    "chat  → 7  \n",
    "dort  → 4\n",
    "\n",
    "On simule donc :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14cdb8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs : tensor([2, 7, 4])\n"
     ]
    }
   ],
   "source": [
    "# Phrase simulée\n",
    "token_ids = torch.tensor([2, 7, 4])\n",
    "print(\"Token IDs :\", token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c721239",
   "metadata": {},
   "source": [
    "On crée une couche d'embedding pour les tokens : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ff8a547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token embeddings :\n",
      "tensor([[-0.7521,  1.6487, -0.3925, -1.4036],\n",
      "        [-0.2316,  0.0418, -0.2516,  0.8599],\n",
      "        [ 1.6423, -0.1596, -0.4974,  0.4396]], grad_fn=<EmbeddingBackward0>)\n",
      "Shape : torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 20\n",
    "embedding_dim = 4\n",
    "\n",
    "torch.manual_seed(42)\n",
    "token_embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "token_embeddings = token_embedding_layer(token_ids)\n",
    "\n",
    "print(\"Token embeddings :\")\n",
    "print(token_embeddings)\n",
    "print(\"Shape :\", token_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d812c9",
   "metadata": {},
   "source": [
    "À ce stade :\n",
    "\n",
    "Chaque mot a un vecteur.\n",
    "Mais rien dans ces vecteurs n’indique que :\n",
    "\n",
    "\"Le\" est en position 0\n",
    "\n",
    "\"chat\" est en position 1\n",
    "\n",
    "\"dort\" est en position 2\n",
    "\n",
    "Si on mélange l’ordre : `dort chat Le` alors les embeddings seraient les mêmes, juste permutés.\n",
    "\n",
    "Le modèle ne comprendrait pas la différence structurelle. On crée donc une seconde couche d’embedding pour représenter les positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff6795c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position embeddings :\n",
      "tensor([[ 0.4679, -0.2049, -0.7409,  0.3618],\n",
      "        [ 1.9199, -0.2254, -0.3417,  0.3040],\n",
      "        [-0.6890, -1.1267, -0.2858, -1.0935]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "sequence_length = len(token_ids)\n",
    "\n",
    "position_embedding_layer = nn.Embedding(sequence_length, embedding_dim)\n",
    "\n",
    "position_ids = torch.arange(sequence_length)\n",
    "\n",
    "position_embeddings = position_embedding_layer(position_ids)\n",
    "\n",
    "print(\"Position embeddings :\")\n",
    "print(position_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e769fb7",
   "metadata": {},
   "source": [
    "Ici :\n",
    "\n",
    "* Position 0 a son propre vecteur\n",
    "* Position 1 a son propre vecteur\n",
    "* Position 2 a son propre vecteur\n",
    "\n",
    "Et on peut donc combiner les deux informations : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "be02b0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings finaux envoyés au Transformer :\n",
      "tensor([[-0.2842,  1.4438, -1.1334, -1.0418],\n",
      "        [ 1.6883, -0.1836, -0.5933,  1.1639],\n",
      "        [ 0.9533, -1.2863, -0.7832, -0.6539]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input_embeddings = token_embeddings + position_embeddings\n",
    "\n",
    "print(\"Embeddings finaux envoyés au Transformer :\")\n",
    "print(input_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1c9e06",
   "metadata": {},
   "source": [
    "Maintenant chaque vecteur encode :\n",
    "\n",
    "* le sens du mot\n",
    "* sa position dans la phrase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e228ab85",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
